2025-03-04 01:13:46,410 [INFO]  * Restarting with stat
2025-03-04 01:14:53,877 [WARNING]  * Debugger is active!
2025-03-04 01:14:53,942 [INFO]  * Debugger PIN: 811-561-076
2025-03-04 01:14:54,179 [WARNING]  * Running on all addresses.
   WARNING: This is a development server. Do not use it in a production deployment.
2025-03-04 01:14:54,179 [INFO]  * Running on http://192.168.5.184:5000/ (Press CTRL+C to quit)
2025-03-04 01:33:49,838 [INFO] 127.0.0.1 - - [04/Mar/2025 01:33:49] "GET /health HTTP/1.1" 200 -
2025-03-04 01:33:49,961 [INFO] 127.0.0.1 - - [04/Mar/2025 01:33:49] "[33mGET /favicon.ico HTTP/1.1[0m" 404 -
2025-03-04 01:36:44,016 [INFO] 127.0.0.1 - - [04/Mar/2025 01:36:44] "OPTIONS /api/chat HTTP/1.1" 200 -
2025-03-04 01:36:44,283 [INFO] Received query: What are my rights as a tenant?
2025-03-04 01:36:47,434 [INFO] Detected language: en
2025-03-04 01:36:48,240 [INFO] Retrieved 3 documents
2025-03-04 01:36:54,914 [ERROR] Error processing query: Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install 'accelerate>=0.26.0'`
Traceback (most recent call last):
  File "F:\LegaLuna\backend\app.py", line 42, in chat
    response = answer_legal_question(query)
  File "F:\LegaLuna\backend\rag_system.py", line 89, in answer_legal_question
    model_response = llm.generate_response(prompt)
  File "F:\LegaLuna\backend\model_loader.py", line 33, in generate_response
    self.load_model()
  File "F:\LegaLuna\backend\model_loader.py", line 20, in load_model
    self.model = AutoModelForCausalLM.from_pretrained(
  File "F:\LegaLuna\venv\lib\site-packages\transformers\models\auto\auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
  File "F:\LegaLuna\venv\lib\site-packages\transformers\modeling_utils.py", line 262, in _wrapper
    return func(*args, **kwargs)
  File "F:\LegaLuna\venv\lib\site-packages\transformers\modeling_utils.py", line 3611, in from_pretrained
    raise ImportError(
ImportError: Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install 'accelerate>=0.26.0'`
2025-03-04 01:36:55,048 [INFO] 127.0.0.1 - - [04/Mar/2025 01:36:55] "[35m[1mPOST /api/chat HTTP/1.1[0m" 500 -
2025-03-04 01:38:28,237 [INFO] 127.0.0.1 - - [04/Mar/2025 01:38:28] "OPTIONS /api/chat HTTP/1.1" 200 -
2025-03-04 01:38:28,489 [INFO] Received query: What are my rights as a tenant?
2025-03-04 01:38:28,489 [INFO] Detected language: en
2025-03-04 01:38:28,653 [INFO] Retrieved 3 documents
2025-03-04 01:38:29,921 [ERROR] Error processing query: Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install 'accelerate>=0.26.0'`
Traceback (most recent call last):
  File "F:\LegaLuna\backend\app.py", line 42, in chat
    response = answer_legal_question(query)
  File "F:\LegaLuna\backend\rag_system.py", line 89, in answer_legal_question
    model_response = llm.generate_response(prompt)
  File "F:\LegaLuna\backend\model_loader.py", line 33, in generate_response
    self.load_model()
  File "F:\LegaLuna\backend\model_loader.py", line 20, in load_model
    self.model = AutoModelForCausalLM.from_pretrained(
  File "F:\LegaLuna\venv\lib\site-packages\transformers\models\auto\auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
  File "F:\LegaLuna\venv\lib\site-packages\transformers\modeling_utils.py", line 262, in _wrapper
    return func(*args, **kwargs)
  File "F:\LegaLuna\venv\lib\site-packages\transformers\modeling_utils.py", line 3611, in from_pretrained
    raise ImportError(
ImportError: Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install 'accelerate>=0.26.0'`
2025-03-04 01:38:29,921 [INFO] 127.0.0.1 - - [04/Mar/2025 01:38:29] "[35m[1mPOST /api/chat HTTP/1.1[0m" 500 -
2025-03-04 01:50:11,116 [INFO] 127.0.0.1 - - [04/Mar/2025 01:50:11] "[33mGET / HTTP/1.1[0m" 404 -
2025-03-04 01:50:19,038 [INFO] 127.0.0.1 - - [04/Mar/2025 01:50:19] "GET /health HTTP/1.1" 200 -
2025-03-04 02:09:49,599 [INFO]  * Restarting with stat
2025-03-04 02:10:40,741 [WARNING]  * Debugger is active!
2025-03-04 02:10:40,788 [INFO]  * Debugger PIN: 811-561-076
2025-03-04 02:10:40,981 [WARNING]  * Running on all addresses.
   WARNING: This is a development server. Do not use it in a production deployment.
2025-03-04 02:10:40,981 [INFO]  * Running on http://192.168.5.184:5000/ (Press CTRL+C to quit)
2025-03-04 02:17:53,000 [INFO] 127.0.0.1 - - [04/Mar/2025 02:17:53] "GET /health HTTP/1.1" 200 -
2025-03-04 02:18:13,303 [INFO] 127.0.0.1 - - [04/Mar/2025 02:18:13] "OPTIONS /api/chat HTTP/1.1" 200 -
2025-03-04 02:18:13,560 [INFO] Received query: How do I file an FIR?
2025-03-04 02:18:14,975 [INFO] Detected language: en
2025-03-04 02:18:15,356 [INFO] Retrieved 3 documents
2025-03-04 02:18:16,945 [ERROR] Error processing query: Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`
Traceback (most recent call last):
  File "F:\LegaLuna\backend\app.py", line 42, in chat
    response = answer_legal_question(query)
  File "F:\LegaLuna\backend\rag_system.py", line 89, in answer_legal_question
    model_response = llm.generate_response(prompt)
  File "F:\LegaLuna\backend\model_loader.py", line 33, in generate_response
    self.load_model()
  File "F:\LegaLuna\backend\model_loader.py", line 20, in load_model
    self.model = AutoModelForCausalLM.from_pretrained(
  File "F:\LegaLuna\venv\lib\site-packages\transformers\models\auto\auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
  File "F:\LegaLuna\venv\lib\site-packages\transformers\modeling_utils.py", line 262, in _wrapper
    return func(*args, **kwargs)
  File "F:\LegaLuna\venv\lib\site-packages\transformers\modeling_utils.py", line 3698, in from_pretrained
    hf_quantizer.validate_environment(
  File "F:\LegaLuna\venv\lib\site-packages\transformers\quantizers\quantizer_bnb_4bit.py", line 75, in validate_environment
    raise ImportError(
ImportError: Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`
2025-03-04 02:18:17,102 [INFO] 127.0.0.1 - - [04/Mar/2025 02:18:17] "[35m[1mPOST /api/chat HTTP/1.1[0m" 500 -
2025-03-04 03:02:55,833 [INFO]  * Restarting with stat
2025-03-04 03:03:52,370 [WARNING]  * Debugger is active!
2025-03-04 03:03:52,448 [INFO]  * Debugger PIN: 811-561-076
2025-03-04 03:03:52,780 [WARNING]  * Running on all addresses.
   WARNING: This is a development server. Do not use it in a production deployment.
2025-03-04 03:03:52,780 [INFO]  * Running on http://192.168.5.184:5000/ (Press CTRL+C to quit)
2025-03-04 03:09:39,507 [INFO] 127.0.0.1 - - [04/Mar/2025 03:09:39] "GET /health HTTP/1.1" 200 -
2025-03-04 03:09:40,839 [INFO] 127.0.0.1 - - [04/Mar/2025 03:09:40] "GET /health HTTP/1.1" 200 -
2025-03-04 03:09:41,029 [INFO] 127.0.0.1 - - [04/Mar/2025 03:09:41] "GET /health HTTP/1.1" 200 -
2025-03-04 03:11:05,437 [INFO] 127.0.0.1 - - [04/Mar/2025 03:11:05] "GET /health HTTP/1.1" 200 -
2025-03-04 03:11:24,102 [INFO] 127.0.0.1 - - [04/Mar/2025 03:11:24] "OPTIONS /api/chat HTTP/1.1" 200 -
2025-03-04 03:11:24,367 [INFO] Received query: How do I file an FIR?
2025-03-04 03:11:25,183 [INFO] Detected language: en
2025-03-04 03:11:25,616 [INFO] Retrieved 3 documents
2025-03-04 03:11:27,366 [ERROR] Error processing query: model_name_or_path is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`
Traceback (most recent call last):
  File "F:\LegaLuna\venv\lib\site-packages\huggingface_hub\utils\_http.py", line 409, in hf_raise_for_status
    response.raise_for_status()
  File "F:\LegaLuna\venv\lib\site-packages\requests\models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/model_name_or_path/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "F:\LegaLuna\venv\lib\site-packages\transformers\utils\hub.py", line 342, in cached_file
    resolved_file = hf_hub_download(
  File "F:\LegaLuna\venv\lib\site-packages\huggingface_hub\utils\_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "F:\LegaLuna\venv\lib\site-packages\huggingface_hub\file_download.py", line 862, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
  File "F:\LegaLuna\venv\lib\site-packages\huggingface_hub\file_download.py", line 969, in _hf_hub_download_to_cache_dir
    _raise_on_head_call_error(head_call_error, force_download, local_files_only)
  File "F:\LegaLuna\venv\lib\site-packages\huggingface_hub\file_download.py", line 1486, in _raise_on_head_call_error
    raise head_call_error
  File "F:\LegaLuna\venv\lib\site-packages\huggingface_hub\file_download.py", line 1376, in _get_metadata_or_catch_error
    metadata = get_hf_file_metadata(
  File "F:\LegaLuna\venv\lib\site-packages\huggingface_hub\utils\_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "F:\LegaLuna\venv\lib\site-packages\huggingface_hub\file_download.py", line 1296, in get_hf_file_metadata
    r = _request_wrapper(
  File "F:\LegaLuna\venv\lib\site-packages\huggingface_hub\file_download.py", line 280, in _request_wrapper
    response = _request_wrapper(
  File "F:\LegaLuna\venv\lib\site-packages\huggingface_hub\file_download.py", line 304, in _request_wrapper
    hf_raise_for_status(response)
  File "F:\LegaLuna\venv\lib\site-packages\huggingface_hub\utils\_http.py", line 458, in hf_raise_for_status
    raise _format(RepositoryNotFoundError, message, response) from e
huggingface_hub.errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-67c62207-2165e68160fb2d352e37c96f;5a39af41-7b5a-4a3c-a98a-39a29c7a502d)

Repository Not Found for url: https://huggingface.co/model_name_or_path/resolve/main/config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "F:\LegaLuna\backend\app.py", line 42, in chat
    response = answer_legal_question(query)
  File "F:\LegaLuna\backend\rag_system.py", line 89, in answer_legal_question
    model_response = llm.generate_response(prompt)
  File "F:\LegaLuna\backend\model_loader.py", line 39, in generate_response
    self.load_model()
  File "F:\LegaLuna\backend\model_loader.py", line 28, in load_model
    self.model = AutoModelForCausalLM.from_pretrained(
  File "F:\LegaLuna\venv\lib\site-packages\transformers\models\auto\auto_factory.py", line 487, in from_pretrained
    resolved_config_file = cached_file(
  File "F:\LegaLuna\venv\lib\site-packages\transformers\utils\hub.py", line 365, in cached_file
    raise EnvironmentError(
OSError: model_name_or_path is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`
2025-03-04 03:11:27,650 [INFO] 127.0.0.1 - - [04/Mar/2025 03:11:27] "[35m[1mPOST /api/chat HTTP/1.1[0m" 500 -
2025-03-04 07:16:35,635 [INFO]  * Restarting with stat
2025-03-04 07:17:26,689 [WARNING]  * Debugger is active!
2025-03-04 07:17:26,727 [INFO]  * Debugger PIN: 811-561-076
2025-03-04 07:17:26,901 [WARNING]  * Running on all addresses.
   WARNING: This is a development server. Do not use it in a production deployment.
2025-03-04 07:17:26,901 [INFO]  * Running on http://192.168.5.184:5000/ (Press CTRL+C to quit)
2025-03-04 07:21:38,324 [INFO] 192.168.5.184 - - [04/Mar/2025 07:21:38] "[33mGET / HTTP/1.1[0m" 404 -
2025-03-04 07:21:38,451 [INFO] 192.168.5.184 - - [04/Mar/2025 07:21:38] "[33mGET /favicon.ico HTTP/1.1[0m" 404 -
2025-03-04 07:22:02,026 [INFO] 192.168.5.184 - - [04/Mar/2025 07:22:02] "[33mGET / HTTP/1.1[0m" 404 -
2025-03-04 07:22:12,712 [INFO] 127.0.0.1 - - [04/Mar/2025 07:22:12] "GET /health HTTP/1.1" 200 -
2025-03-04 07:22:13,927 [INFO] 127.0.0.1 - - [04/Mar/2025 07:22:13] "GET /health HTTP/1.1" 200 -
2025-03-04 07:22:14,227 [INFO] 127.0.0.1 - - [04/Mar/2025 07:22:14] "GET /health HTTP/1.1" 200 -
2025-03-04 07:22:20,927 [INFO] 127.0.0.1 - - [04/Mar/2025 07:22:20] "OPTIONS /api/chat HTTP/1.1" 200 -
2025-03-04 07:22:21,194 [INFO] Received query: How do I file an FIR?
2025-03-04 07:22:22,109 [INFO] Detected language: en
2025-03-04 07:22:22,360 [INFO] Retrieved 3 documents
2025-03-04 07:22:24,975 [WARNING] The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.
2025-03-04 07:22:25,108 [ERROR] Error processing query: CUDA is required but not available for bitsandbytes. Please consider installing the multi-platform enabled version of bitsandbytes, which is currently a work in progress. Please check currently supported platforms and installation instructions at https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend
Traceback (most recent call last):
  File "F:\LegaLuna\backend\app.py", line 42, in chat
    response = answer_legal_question(query)
  File "F:\LegaLuna\backend\rag_system.py", line 89, in answer_legal_question
    model_response = llm.generate_response(prompt)
  File "F:\LegaLuna\backend\model_loader.py", line 41, in generate_response
    self.load_model()
  File "F:\LegaLuna\backend\model_loader.py", line 29, in load_model
    self.model = AutoModelForCausalLM.from_pretrained(
  File "F:\LegaLuna\venv\lib\site-packages\transformers\models\auto\auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
  File "F:\LegaLuna\venv\lib\site-packages\transformers\modeling_utils.py", line 262, in _wrapper
    return func(*args, **kwargs)
  File "F:\LegaLuna\venv\lib\site-packages\transformers\modeling_utils.py", line 3698, in from_pretrained
    hf_quantizer.validate_environment(
  File "F:\LegaLuna\venv\lib\site-packages\transformers\quantizers\quantizer_bnb_4bit.py", line 83, in validate_environment
    validate_bnb_backend_availability(raise_exception=True)
  File "F:\LegaLuna\venv\lib\site-packages\transformers\integrations\bitsandbytes.py", line 559, in validate_bnb_backend_availability
    return _validate_bnb_cuda_backend_availability(raise_exception)
  File "F:\LegaLuna\venv\lib\site-packages\transformers\integrations\bitsandbytes.py", line 537, in _validate_bnb_cuda_backend_availability
    raise RuntimeError(log_msg)
RuntimeError: CUDA is required but not available for bitsandbytes. Please consider installing the multi-platform enabled version of bitsandbytes, which is currently a work in progress. Please check currently supported platforms and installation instructions at https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend
2025-03-04 07:22:25,258 [INFO] 127.0.0.1 - - [04/Mar/2025 07:22:25] "[35m[1mPOST /api/chat HTTP/1.1[0m" 500 -
